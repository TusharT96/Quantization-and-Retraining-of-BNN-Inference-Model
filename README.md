# Quantization-and-Retraining-of-BNN-Inference-Model


Deep Neural Networks (DNNs) have gained wide-spread importance for many Artificial
Intelligence(AI) applications, Speech Recognition, Robotics and Computer Vision. The demand for
smart/intelligent mobile devices, the deterring computational cost and memory requirements in the
case of deep neural networks based models are increasing with its wide-spread real-time and nonreal time applications. Thus, the project focuses on more accurate and efficient inference schemes
by quantization and training. A quantized inference framework that efficiently is able to implement
mixed-arithmetic and that handles zero-points is proposed. Also, the approach includes saving the
accuracy of relatively smaller model size and decreasing the deep neural network size to relatively
smaller size, called as Quantization. The proposed quantized inference schemes will help improve
the efficiency by restoring the model accuracy to the close levels as that of the native one with
smaller model size. The changes in the efficiency must be evident even on MobileNets which is a
well-known model for run-time efficiency.


KEYWORDS â€“ Quantization, inference framework, training, efficiency, deep neural networks,
mixed-arithmetic
